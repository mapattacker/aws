{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Guide This documentation serves as a collection of AWS guides for my own consumption. Hope it can help you in the journey to master how to use services in this dominant cloud provider.","title":"Home"},{"location":"#aws-guide","text":"This documentation serves as a collection of AWS guides for my own consumption. Hope it can help you in the journey to master how to use services in this dominant cloud provider.","title":"AWS Guide"},{"location":"api-gateway/","text":"API Gateway API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. A gateway to AWS services Models & Mappings We can use API Gateway as a proxy for our backend. If we use REST API Gateway we can: Validate requests & responses with models written in JSON done at Method Request/Response Transform requests & responses with mappings written in VTL language done at Integration Request/Response Method & Integration for Requests & Responses Deployment Once a REST API is created in API Gateway, it doesn\u2019t automatically become available to invoke. We need to publish the API first. Deploying the API In API Gateway, we publish the API to a stage. Indicate deployment stage The API endpoint will be appended with the stage path, e.g., for dev stage, it would be https://a69m13u8y3.execute-api.ap-southeast-1.amazonaws.com/dev . For each stage, we can add configurations to it, e.g., throttling, logs, or canary deployment. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance API Authorization Some ways we can authorize and authenticate users of our API CORS By default, if the API is executed from a different domain, this will be blocked by AWS. However, we can enable it in API-Gateway by enabling Cross Origin Resource Sharing (CORS). Allowing CORS API-Key An API-key is a password that is generated to an API-user. With this, only the correct api-key have access to the API. However, as it is a password string sent within a request, it is not very secure. The upside of this method is that, we can tag a usage plan for a particular API-key, including the quota (# request/mth) and throttling (# request/sec). First we will need to create an API key. Copy the API key generated. The key is retrieval, unlike your AWS credentials which can only be generated once. Then, we will need to create a usage plan and add the API Key to that plan. Once that is done, we will need to assign an API Stage & Method with this plan. Last, we will need to go to the Resouces > click on the API method > Method Request > Settings > change the API Key Required to true. Once that is done, we redeploy the API. Change API Key Required to True Lastly, we can try accessing the API by adding it as a header in the request with the key name x-api-key . IP Whitelisting We can use the API gateway resource policy to limit access to specific IP addresses. An example below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"execute-api:Invoke\" , \"Resource\" : \"execute-api:/*\" , \"Condition\" : { \"IpAddress\" : { \"aws:SourceIp\" : [ \"1.2.3.4/32\" ] } } } ] } VPC Endpoint Policy For private APIs, we can improve the security by configuring the policy at the VPC endpoints. Cognito AWS Cognito consists of user pool & identity pool, which are two separate services. User pool is a user directory in Cognito, and it also provides a UI for users to sign into your app. If we only need to authorize an API gateway, we can just use the user-pool. This involves signing in to cognito via their UI and obtaining a JSON Web Token (JWT). However, if we require access to other AWS services besides just API gateway, we will need to use an identity pool.","title":"API Gateway"},{"location":"api-gateway/#api-gateway","text":"API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. A gateway to AWS services","title":"API Gateway"},{"location":"api-gateway/#models-mappings","text":"We can use API Gateway as a proxy for our backend. If we use REST API Gateway we can: Validate requests & responses with models written in JSON done at Method Request/Response Transform requests & responses with mappings written in VTL language done at Integration Request/Response Method & Integration for Requests & Responses","title":"Models &amp; Mappings"},{"location":"api-gateway/#deployment","text":"Once a REST API is created in API Gateway, it doesn\u2019t automatically become available to invoke. We need to publish the API first. Deploying the API In API Gateway, we publish the API to a stage. Indicate deployment stage The API endpoint will be appended with the stage path, e.g., for dev stage, it would be https://a69m13u8y3.execute-api.ap-southeast-1.amazonaws.com/dev . For each stage, we can add configurations to it, e.g., throttling, logs, or canary deployment. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance","title":"Deployment"},{"location":"api-gateway/#api-authorization","text":"Some ways we can authorize and authenticate users of our API","title":"API Authorization"},{"location":"api-gateway/#cors","text":"By default, if the API is executed from a different domain, this will be blocked by AWS. However, we can enable it in API-Gateway by enabling Cross Origin Resource Sharing (CORS). Allowing CORS","title":"CORS"},{"location":"api-gateway/#api-key","text":"An API-key is a password that is generated to an API-user. With this, only the correct api-key have access to the API. However, as it is a password string sent within a request, it is not very secure. The upside of this method is that, we can tag a usage plan for a particular API-key, including the quota (# request/mth) and throttling (# request/sec). First we will need to create an API key. Copy the API key generated. The key is retrieval, unlike your AWS credentials which can only be generated once. Then, we will need to create a usage plan and add the API Key to that plan. Once that is done, we will need to assign an API Stage & Method with this plan. Last, we will need to go to the Resouces > click on the API method > Method Request > Settings > change the API Key Required to true. Once that is done, we redeploy the API. Change API Key Required to True Lastly, we can try accessing the API by adding it as a header in the request with the key name x-api-key .","title":"API-Key"},{"location":"api-gateway/#ip-whitelisting","text":"We can use the API gateway resource policy to limit access to specific IP addresses. An example below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"execute-api:Invoke\" , \"Resource\" : \"execute-api:/*\" , \"Condition\" : { \"IpAddress\" : { \"aws:SourceIp\" : [ \"1.2.3.4/32\" ] } } } ] }","title":"IP Whitelisting"},{"location":"api-gateway/#vpc-endpoint-policy","text":"For private APIs, we can improve the security by configuring the policy at the VPC endpoints.","title":"VPC Endpoint Policy"},{"location":"api-gateway/#cognito","text":"AWS Cognito consists of user pool & identity pool, which are two separate services. User pool is a user directory in Cognito, and it also provides a UI for users to sign into your app. If we only need to authorize an API gateway, we can just use the user-pool. This involves signing in to cognito via their UI and obtaining a JSON Web Token (JWT). However, if we require access to other AWS services besides just API gateway, we will need to use an identity pool.","title":"Cognito"},{"location":"boto3/","text":"Boto3 Boto3 is the AWS SDK for Python. To get started, you need to have python installed. pip install boto3 Boto3 Types Boto3 has two types of interfaces, the client and resource. The client interface is low level and provides 1-1 mapping with the AWS services' APIs, with the return responses in JSON. All service operations are supported by clients. The resource interface is a high level API, a wrapper for the client interface so that commands are more intuitive. Below is an example to download a file from S3. import boto3 def download_file ( bucket_name , origin_blob_path , dest_filename ): \"\"\"Download blob from S3 bucket. Args: bucket_name (str) origin_blob_path (str) dest_filename (str): destination filename \"\"\" s3 = boto3 . resource ( \"s3\" ) s3_bucket = s3 . Bucket ( bucket_name ) s3 . Bucket ( bucket_name ) . download_file ( origin_blob_path , dest_filename ) However, it only exposes a subset of AWS API, so functionalities might be limited , though we can assess the client interface in the resouce too as shown below. s3 = boto3 . resource ( \"s3\" ) . meta . client () Credentials To access each of the AWS services, we will need to pass our credentials either as arguments into the client or resource interfaces (not recommended!), or the SDK can detect them within the env variables, or lastly if u set them in your aws configure . S3 S3 charges for the bandwidth transferred out of S3 if its more than 1GB/mth. One of the ways to reduce this limit is to use S3 Select where you send a filter that","title":"Boto3"},{"location":"boto3/#boto3","text":"Boto3 is the AWS SDK for Python. To get started, you need to have python installed. pip install boto3","title":"Boto3"},{"location":"boto3/#boto3-types","text":"Boto3 has two types of interfaces, the client and resource. The client interface is low level and provides 1-1 mapping with the AWS services' APIs, with the return responses in JSON. All service operations are supported by clients. The resource interface is a high level API, a wrapper for the client interface so that commands are more intuitive. Below is an example to download a file from S3. import boto3 def download_file ( bucket_name , origin_blob_path , dest_filename ): \"\"\"Download blob from S3 bucket. Args: bucket_name (str) origin_blob_path (str) dest_filename (str): destination filename \"\"\" s3 = boto3 . resource ( \"s3\" ) s3_bucket = s3 . Bucket ( bucket_name ) s3 . Bucket ( bucket_name ) . download_file ( origin_blob_path , dest_filename ) However, it only exposes a subset of AWS API, so functionalities might be limited , though we can assess the client interface in the resouce too as shown below. s3 = boto3 . resource ( \"s3\" ) . meta . client ()","title":"Boto3 Types"},{"location":"boto3/#credentials","text":"To access each of the AWS services, we will need to pass our credentials either as arguments into the client or resource interfaces (not recommended!), or the SDK can detect them within the env variables, or lastly if u set them in your aws configure .","title":"Credentials"},{"location":"boto3/#s3","text":"S3 charges for the bandwidth transferred out of S3 if its more than 1GB/mth. One of the ways to reduce this limit is to use S3 Select where you send a filter that","title":"S3"},{"location":"cli/","text":"AWS CLI The AWS Command Line Interface , AWS CLI for short, allows us to easily access AWS services in the terminal. Installation If you\u2019re using Amazon EC2 instances or AWS Cloud9, the tools are already installed for you. To install in your local machine, refer to this guide . Command Syntax # command syntax aws --<optional-command> <main-command> <subcommand> <parameters> # e.g. aws s3 mb s3://bucketname We can find more info in the online CLI help docs, or just type aws <optional: command> <optional: subcommand> help , e.g. aws s3 mb help if needed. Configure To make it easier to send commands without the need to enter the credentials everytime, we can set a default profile or a specific profile name with those variables. The 4 credentials are the access and secret keys , region , and output format (default is json ). AWS Configure Cmd Desc aws configure enter access, secret & region names, for default profile aws configure --profile <name> enter access, secret & region names, based on a specific name aws s3 ls --profile <name> enter commands based on profile These credentials are stored in ~/.aws/credentials .","title":"AWS CLI"},{"location":"cli/#aws-cli","text":"The AWS Command Line Interface , AWS CLI for short, allows us to easily access AWS services in the terminal.","title":"AWS CLI"},{"location":"cli/#installation","text":"If you\u2019re using Amazon EC2 instances or AWS Cloud9, the tools are already installed for you. To install in your local machine, refer to this guide .","title":"Installation"},{"location":"cli/#command-syntax","text":"# command syntax aws --<optional-command> <main-command> <subcommand> <parameters> # e.g. aws s3 mb s3://bucketname We can find more info in the online CLI help docs, or just type aws <optional: command> <optional: subcommand> help , e.g. aws s3 mb help if needed.","title":"Command Syntax"},{"location":"cli/#configure","text":"To make it easier to send commands without the need to enter the credentials everytime, we can set a default profile or a specific profile name with those variables. The 4 credentials are the access and secret keys , region , and output format (default is json ). AWS Configure Cmd Desc aws configure enter access, secret & region names, for default profile aws configure --profile <name> enter access, secret & region names, based on a specific name aws s3 ls --profile <name> enter commands based on profile These credentials are stored in ~/.aws/credentials .","title":"Configure"},{"location":"cloud9/","text":"Cloud9 Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don\u2019t need to install files or configure your development machine to start new projects. Interface of Cloud9 IDE Starting Cloud9 Cloud9 runs on an EC2 instance, and charges based on the instance type and storage. If we choose the cheapest t2.micro , AWS estimates it to only be USD$2.05 per month. Free tier AWS (first year of account opening) has 750 free hours for EC2, so the costs should be either neligible or none at all. We should allow the cost saving settings to be on to prevent ballooning accidental costs. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance To close Cloud9 IDE, we need to go to the EC2 instance to shutdown. To start the instance, we can just go to Cloud9 service in management console and open the IDE. Credentials By default, Cloud9 has temporary credentials that will refresh every 5 mins. These credentials are the same as the IAM role that was used to create this Cloud9 instance, so the permissions to AWS resources are the same. To use a different credentials, we need to go to Cloud9 Settings > AWS Settings > Credentials > turn off AWS managed temporary credentials. Then in the terminal, AWS Configure & enter your other credentials. Switching off Temp Credentials File Uploads We can upload files & access the settings from the Welcome page.","title":"Cloud9"},{"location":"cloud9/#cloud9","text":"Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don\u2019t need to install files or configure your development machine to start new projects. Interface of Cloud9 IDE","title":"Cloud9"},{"location":"cloud9/#starting-cloud9","text":"Cloud9 runs on an EC2 instance, and charges based on the instance type and storage. If we choose the cheapest t2.micro , AWS estimates it to only be USD$2.05 per month. Free tier AWS (first year of account opening) has 750 free hours for EC2, so the costs should be either neligible or none at all. We should allow the cost saving settings to be on to prevent ballooning accidental costs. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance To close Cloud9 IDE, we need to go to the EC2 instance to shutdown. To start the instance, we can just go to Cloud9 service in management console and open the IDE.","title":"Starting Cloud9"},{"location":"cloud9/#credentials","text":"By default, Cloud9 has temporary credentials that will refresh every 5 mins. These credentials are the same as the IAM role that was used to create this Cloud9 instance, so the permissions to AWS resources are the same. To use a different credentials, we need to go to Cloud9 Settings > AWS Settings > Credentials > turn off AWS managed temporary credentials. Then in the terminal, AWS Configure & enter your other credentials. Switching off Temp Credentials","title":"Credentials"},{"location":"cloud9/#file-uploads","text":"We can upload files & access the settings from the Welcome page.","title":"File Uploads"},{"location":"iam/","text":"IAM AWS Identity and Access Management ( IAM ) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. Root User When we first create a AWS account, it will be created as a root user, having full access over this account. It will be wise to create a user for future logins for security reasons. Principle of Least Privilege An important security recommendation is the principle of least privilege, where we only assign policies to a resource or person based on what they require only.","title":"IAM"},{"location":"iam/#iam","text":"AWS Identity and Access Management ( IAM ) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.","title":"IAM"},{"location":"iam/#root-user","text":"When we first create a AWS account, it will be created as a root user, having full access over this account. It will be wise to create a user for future logins for security reasons.","title":"Root User"},{"location":"iam/#principle-of-least-privilege","text":"An important security recommendation is the principle of least privilege, where we only assign policies to a resource or person based on what they require only.","title":"Principle of Least Privilege"},{"location":"lambda/","text":"Lambda AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. We can upload our code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic. We can also set up your code to automatically trigger from over 200 AWS services and SaaS applications or call it directly from any web or mobile app. Execution Environment It takes time to set up an execution context and do the necessary \"bootstrapping\", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. This is known as cold start . After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. This is known as execution context reuse . The execution context is kept warm 15mins after the end of an execution, before cold starting again. To keep your lambda warm, we can have a scheduled job in Cloudwatch to involve the API every 15mins. This setting is also available in tools like zappa & serverless. To keep multiples of the same lambdas always warm, we can set a lambda provisioned concurrency setting , but there will be an add-on charge. Objects declared outside of the function's handler method remain initialized, providing additional optimization when the function is invoked again . For example, if your Lambda function establishes a database connection, instead of reestablishing the connection, the original connection is used in subsequent invocations. However, it is recommended to add logic in the code to check if a connection exists before creating one. Each execution context provides 512 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. For information on deployment limits, see AWS Lambda limits. Create Lambda Handler In a python handler, it includes the handler name (function name), event , which is the information passing from the request, and context , which contains AWS specific invocations or execution of your lambda, e.g. memory, cognito identity details etc. def handler_name ( event , context ): # do something return something Zip libraries & handler We will need to zip both python packages and the handler... # Download dependencies into folder sudo pip install --target ./add-dragon-package boto3 # Zip up your code (with dependencies) zip -r9 ${ OLDPWD } / pythonAddDragonFunction.zip . # Add python script to zip zip -g pythonAddDragonFunction.zip addDragon.py Create Lambda Then upload the zip file using either... Using AWS CLI, or # Create Lambda Function aws lambda create-function --function-name add-dragon --runtime python3.6 --role <IAM ROLE ARN> --handler addDragon.addDragonToFile --publish --zip-file fileb://pythonAddDragonFunction.zip # Invoke Lambda Function aws lambda invoke --function-name add-dragon output.txt --payload file://newDragonPayload.json # Update Lambda Code aws lambda update-function-code --function-name add-dragon --zip-file fileb://pythonAddDragonFunction.zip --publish Using management console. Basic Configs Some important settings for lambda include the memory, which also determines the number of CPU, timeout, and execution roles. Others include concurrency, which limits the number of lambdas that will be spun up at a given time, and environment variables, which your function can access. Permissions There are two types of permissions for lambda. The first is execution permissions , which defines what the lambda can do. This is set up in an IAM role. The most basic policy is AWSLambdaBasicExecutionRole , which is allows permission to upload logs to CloudWatch. Some other lambda configured policies are included here . The second type of permission is resource permissions , which defines what can invoke or manage your lambda functions. This is set by triggers. Triggers Lambda functions can be invoked by the push or pull models. For the push model, for example, someone can send a request to the API Gateway, which triggers the lambda to run. Alternatively, someone can upload a file to S3, and it triggers a lambda to process that file. For the pull model, for example, lambda will look for data in streaming or queue services like SQS, Kinesis, DynamoDB Streams, and trigger to pull the data into the function. This is configured through event source mappings. Version & Alias You can publish a new version of your AWS Lambda function when you create new or update existing functions. Each version of a lambda function gets it\u2019s own unique Amazon Resource Name (ARN). You can then use these ARNs to use different versions of the Lambda function for different purposes. You also have the ability to create aliases for AWS Lambda functions. Aliases are essentially pointers to one specific Lambda version.","title":"Lambda"},{"location":"lambda/#lambda","text":"AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. We can upload our code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic. We can also set up your code to automatically trigger from over 200 AWS services and SaaS applications or call it directly from any web or mobile app.","title":"Lambda"},{"location":"lambda/#execution-environment","text":"It takes time to set up an execution context and do the necessary \"bootstrapping\", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. This is known as cold start . After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. This is known as execution context reuse . The execution context is kept warm 15mins after the end of an execution, before cold starting again. To keep your lambda warm, we can have a scheduled job in Cloudwatch to involve the API every 15mins. This setting is also available in tools like zappa & serverless. To keep multiples of the same lambdas always warm, we can set a lambda provisioned concurrency setting , but there will be an add-on charge. Objects declared outside of the function's handler method remain initialized, providing additional optimization when the function is invoked again . For example, if your Lambda function establishes a database connection, instead of reestablishing the connection, the original connection is used in subsequent invocations. However, it is recommended to add logic in the code to check if a connection exists before creating one. Each execution context provides 512 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. For information on deployment limits, see AWS Lambda limits.","title":"Execution Environment"},{"location":"lambda/#create-lambda","text":"","title":"Create Lambda"},{"location":"lambda/#handler","text":"In a python handler, it includes the handler name (function name), event , which is the information passing from the request, and context , which contains AWS specific invocations or execution of your lambda, e.g. memory, cognito identity details etc. def handler_name ( event , context ): # do something return something","title":"Handler"},{"location":"lambda/#zip-libraries-handler","text":"We will need to zip both python packages and the handler... # Download dependencies into folder sudo pip install --target ./add-dragon-package boto3 # Zip up your code (with dependencies) zip -r9 ${ OLDPWD } / pythonAddDragonFunction.zip . # Add python script to zip zip -g pythonAddDragonFunction.zip addDragon.py","title":"Zip libraries &amp; handler"},{"location":"lambda/#create-lambda_1","text":"Then upload the zip file using either... Using AWS CLI, or # Create Lambda Function aws lambda create-function --function-name add-dragon --runtime python3.6 --role <IAM ROLE ARN> --handler addDragon.addDragonToFile --publish --zip-file fileb://pythonAddDragonFunction.zip # Invoke Lambda Function aws lambda invoke --function-name add-dragon output.txt --payload file://newDragonPayload.json # Update Lambda Code aws lambda update-function-code --function-name add-dragon --zip-file fileb://pythonAddDragonFunction.zip --publish Using management console.","title":"Create Lambda"},{"location":"lambda/#basic-configs","text":"Some important settings for lambda include the memory, which also determines the number of CPU, timeout, and execution roles. Others include concurrency, which limits the number of lambdas that will be spun up at a given time, and environment variables, which your function can access.","title":"Basic Configs"},{"location":"lambda/#permissions","text":"There are two types of permissions for lambda. The first is execution permissions , which defines what the lambda can do. This is set up in an IAM role. The most basic policy is AWSLambdaBasicExecutionRole , which is allows permission to upload logs to CloudWatch. Some other lambda configured policies are included here . The second type of permission is resource permissions , which defines what can invoke or manage your lambda functions. This is set by triggers.","title":"Permissions"},{"location":"lambda/#triggers","text":"Lambda functions can be invoked by the push or pull models. For the push model, for example, someone can send a request to the API Gateway, which triggers the lambda to run. Alternatively, someone can upload a file to S3, and it triggers a lambda to process that file. For the pull model, for example, lambda will look for data in streaming or queue services like SQS, Kinesis, DynamoDB Streams, and trigger to pull the data into the function. This is configured through event source mappings.","title":"Triggers"},{"location":"lambda/#version-alias","text":"You can publish a new version of your AWS Lambda function when you create new or update existing functions. Each version of a lambda function gets it\u2019s own unique Amazon Resource Name (ARN). You can then use these ARNs to use different versions of the Lambda function for different purposes. You also have the ability to create aliases for AWS Lambda functions. Aliases are essentially pointers to one specific Lambda version.","title":"Version &amp; Alias"},{"location":"s3/","text":"S3 AWS S3 stands for Simple Storage Service. SDK helper functions import boto3 s3 = boto3 . resource ( 's3' ) def upload_file ( s3 , local_filepath , bucketname , s3_filepath ): \"\"\"upload a file to S3\"\"\" s3 . meta . client . upload_file ( local_filepath , bucketname , s3_filepath ) print ( f \"upload { local_path } complete\" ) def query_json ( s3 , bucketname , SQL_expression , s3_filepath ) \"\"\"select query from json; query is done in S3 so its much cheaper\"\"\" SQL_expression = \"\"\"select * from S3Object[*][*] as s where s.family_str = 'blue' \"\"\" result = s3 . select_object_content ( Bucket = bucket_name , Key = file_name , ExpressionType = 'SQL' , Expression = SQL_expression , InputSerialization = { 'JSON' : { 'Type' : 'Document' }}, OutputSerialization = { 'JSON' : {}} return result if __name__ == \"__main__\" : local_path = \"/home/dragon_stats_one.json\" bucketname = \"dragon-data\" s3_path = \"dragon_stats_one.json\" upload_file ( s3 , local_path , bucketname , s3_path )","title":"S3"},{"location":"s3/#s3","text":"AWS S3 stands for Simple Storage Service.","title":"S3"},{"location":"s3/#sdk-helper-functions","text":"import boto3 s3 = boto3 . resource ( 's3' ) def upload_file ( s3 , local_filepath , bucketname , s3_filepath ): \"\"\"upload a file to S3\"\"\" s3 . meta . client . upload_file ( local_filepath , bucketname , s3_filepath ) print ( f \"upload { local_path } complete\" ) def query_json ( s3 , bucketname , SQL_expression , s3_filepath ) \"\"\"select query from json; query is done in S3 so its much cheaper\"\"\" SQL_expression = \"\"\"select * from S3Object[*][*] as s where s.family_str = 'blue' \"\"\" result = s3 . select_object_content ( Bucket = bucket_name , Key = file_name , ExpressionType = 'SQL' , Expression = SQL_expression , InputSerialization = { 'JSON' : { 'Type' : 'Document' }}, OutputSerialization = { 'JSON' : {}} return result if __name__ == \"__main__\" : local_path = \"/home/dragon_stats_one.json\" bucketname = \"dragon-data\" s3_path = \"dragon_stats_one.json\" upload_file ( s3 , local_path , bucketname , s3_path )","title":"SDK helper functions"}]}