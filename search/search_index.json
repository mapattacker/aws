{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Guide This documentation serves as a collection of AWS guides for my own consumption. Hope it can help you in the journey to master how to use services in this dominant cloud provider. Some of the resources that I learnt from and used heavily in this site are: AWS Certified Cloud Practitioner AWS Fundamentals Specialization Building Modern Python Applications on AWS","title":"Home"},{"location":"#aws-guide","text":"This documentation serves as a collection of AWS guides for my own consumption. Hope it can help you in the journey to master how to use services in this dominant cloud provider. Some of the resources that I learnt from and used heavily in this site are: AWS Certified Cloud Practitioner AWS Fundamentals Specialization Building Modern Python Applications on AWS","title":"AWS Guide"},{"location":"api-gateway/","text":"API Gateway API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. A gateway to AWS services Models & Mappings We can use API Gateway as a proxy for our backend. If we use REST API Gateway we can: Validate requests & responses with models written in JSON done at Method Request/Response Transform requests & responses with mappings written in VTL language done at Integration Request/Response Method & Integration for Requests & Responses Deployment Once a REST API is created in API Gateway, it doesn\u2019t automatically become available to invoke. We need to publish the API first. Deploying the API In API Gateway, we publish the API to a stage. Indicate deployment stage The API endpoint will be appended with the stage path, e.g., for dev stage, it would be https://a69m13u8y3.execute-api.ap-southeast-1.amazonaws.com/dev . For each stage, we can add configurations to it, e.g., throttling, logs, or canary deployment. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance API Authorization Some ways we can authorize and authenticate users of our API CORS By default, if the API is executed from a different domain, this will be blocked by AWS. However, we can enable it in API-Gateway by enabling Cross Origin Resource Sharing (CORS). Allowing CORS API-Key An API-key is a password that is generated to an API-user. With this, only the correct api-key have access to the API. However, as it is a password string sent within a request, it is not very secure. The upside of this method is that, we can tag a usage plan for a particular API-key, including the quota (# request/mth) and throttling (# request/sec). First we will need to create an API key. Copy the API key generated. The key is retrieval, unlike your AWS credentials which can only be generated once. Then, we will need to create a usage plan and add the API Key to that plan. Once that is done, we will need to assign an API Stage & Method with this plan. Last, we will need to go to the Resouces > click on the API method > Method Request > Settings > change the API Key Required to true. Once that is done, we redeploy the API. Change API Key Required to True Lastly, we can try accessing the API by adding it as a header in the request with the key name x-api-key . IP Whitelisting We can use the API gateway resource policy to limit access to specific IP addresses. An example below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"execute-api:Invoke\" , \"Resource\" : \"execute-api:/*\" , \"Condition\" : { \"IpAddress\" : { \"aws:SourceIp\" : [ \"1.2.3.4/32\" ] } } } ] } VPC Endpoint Policy For private APIs, we can improve the security by configuring the policy at the VPC endpoints. Cognito AWS Cognito consists of user pool & identity pool, which are two separate services. User pool is a user directory in Cognito, and it also provides a UI for users to sign into your app. If we only need to authorize an API gateway, we can just use the user-pool. This involves signing in to cognito via their UI and obtaining a JSON Web Token (JWT). However, if we require access to other AWS services besides just API gateway, we will need to use an identity pool.","title":"API Gateway"},{"location":"api-gateway/#api-gateway","text":"API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. A gateway to AWS services","title":"API Gateway"},{"location":"api-gateway/#models-mappings","text":"We can use API Gateway as a proxy for our backend. If we use REST API Gateway we can: Validate requests & responses with models written in JSON done at Method Request/Response Transform requests & responses with mappings written in VTL language done at Integration Request/Response Method & Integration for Requests & Responses","title":"Models &amp; Mappings"},{"location":"api-gateway/#deployment","text":"Once a REST API is created in API Gateway, it doesn\u2019t automatically become available to invoke. We need to publish the API first. Deploying the API In API Gateway, we publish the API to a stage. Indicate deployment stage The API endpoint will be appended with the stage path, e.g., for dev stage, it would be https://a69m13u8y3.execute-api.ap-southeast-1.amazonaws.com/dev . For each stage, we can add configurations to it, e.g., throttling, logs, or canary deployment. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance","title":"Deployment"},{"location":"api-gateway/#api-authorization","text":"Some ways we can authorize and authenticate users of our API","title":"API Authorization"},{"location":"api-gateway/#cors","text":"By default, if the API is executed from a different domain, this will be blocked by AWS. However, we can enable it in API-Gateway by enabling Cross Origin Resource Sharing (CORS). Allowing CORS","title":"CORS"},{"location":"api-gateway/#api-key","text":"An API-key is a password that is generated to an API-user. With this, only the correct api-key have access to the API. However, as it is a password string sent within a request, it is not very secure. The upside of this method is that, we can tag a usage plan for a particular API-key, including the quota (# request/mth) and throttling (# request/sec). First we will need to create an API key. Copy the API key generated. The key is retrieval, unlike your AWS credentials which can only be generated once. Then, we will need to create a usage plan and add the API Key to that plan. Once that is done, we will need to assign an API Stage & Method with this plan. Last, we will need to go to the Resouces > click on the API method > Method Request > Settings > change the API Key Required to true. Once that is done, we redeploy the API. Change API Key Required to True Lastly, we can try accessing the API by adding it as a header in the request with the key name x-api-key .","title":"API-Key"},{"location":"api-gateway/#ip-whitelisting","text":"We can use the API gateway resource policy to limit access to specific IP addresses. An example below. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"execute-api:Invoke\" , \"Resource\" : \"execute-api:/*\" , \"Condition\" : { \"IpAddress\" : { \"aws:SourceIp\" : [ \"1.2.3.4/32\" ] } } } ] }","title":"IP Whitelisting"},{"location":"api-gateway/#vpc-endpoint-policy","text":"For private APIs, we can improve the security by configuring the policy at the VPC endpoints.","title":"VPC Endpoint Policy"},{"location":"api-gateway/#cognito","text":"AWS Cognito consists of user pool & identity pool, which are two separate services. User pool is a user directory in Cognito, and it also provides a UI for users to sign into your app. If we only need to authorize an API gateway, we can just use the user-pool. This involves signing in to cognito via their UI and obtaining a JSON Web Token (JWT). However, if we require access to other AWS services besides just API gateway, we will need to use an identity pool.","title":"Cognito"},{"location":"aws-infra/","text":"Infrastructure There are data centers in Availability Zones (AZ) and AZs within Regions. Both AZs and Regions have redundancies . Regions Region is a cluster of data centers, or Availability Zones. We should choose regions based on the following, with compliance being the priority. AWS Regions are independent from one another. This means that your data is not replicated from one Region to another, without your explicit consent and authorization. Considerations Desc Compliance with data governance & legal requirements Latency hosted proximity to consumer Pricing varies with regions due to, e.g. tax structure Available Services not all services within a region Regions are represented by codes e.g. Singapore is ap-southeast-1 . In the management console, we can choose the region at the top right corner of the screen. Note that there are some services which are global by nature, while others are region specific. For example, switching to another region, you can still see the same IAM service with your data as it is a global service, while your EC2 instances will not appear as it is region specific. Availability Zones AZ(s) or Availability Zones individual data centres within a region, but have redundancy from each other. Edge Locations Edge Locations are Content Delivery Network (CDN) endpoints of CloudFront. Interacting with AWS API There are 3 ways which we can interact with AWS services, which include the AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS Software Development Kits (AWS SDKs).","title":"Infrastructure"},{"location":"aws-infra/#infrastructure","text":"There are data centers in Availability Zones (AZ) and AZs within Regions. Both AZs and Regions have redundancies .","title":"Infrastructure"},{"location":"aws-infra/#regions","text":"Region is a cluster of data centers, or Availability Zones. We should choose regions based on the following, with compliance being the priority. AWS Regions are independent from one another. This means that your data is not replicated from one Region to another, without your explicit consent and authorization. Considerations Desc Compliance with data governance & legal requirements Latency hosted proximity to consumer Pricing varies with regions due to, e.g. tax structure Available Services not all services within a region Regions are represented by codes e.g. Singapore is ap-southeast-1 . In the management console, we can choose the region at the top right corner of the screen. Note that there are some services which are global by nature, while others are region specific. For example, switching to another region, you can still see the same IAM service with your data as it is a global service, while your EC2 instances will not appear as it is region specific.","title":"Regions"},{"location":"aws-infra/#availability-zones","text":"AZ(s) or Availability Zones individual data centres within a region, but have redundancy from each other.","title":"Availability Zones"},{"location":"aws-infra/#edge-locations","text":"Edge Locations are Content Delivery Network (CDN) endpoints of CloudFront.","title":"Edge Locations"},{"location":"aws-infra/#interacting-with-aws-api","text":"There are 3 ways which we can interact with AWS services, which include the AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS Software Development Kits (AWS SDKs).","title":"Interacting with AWS API"},{"location":"billing/","text":"Billing AWS Cost Explorer Allows us to visualize usage patterns over time and to identify underlying cost drivers. AWS Budgets","title":"Billing"},{"location":"billing/#billing","text":"","title":"Billing"},{"location":"billing/#aws-cost-explorer","text":"Allows us to visualize usage patterns over time and to identify underlying cost drivers.","title":"AWS Cost Explorer"},{"location":"billing/#aws-budgets","text":"","title":"AWS Budgets"},{"location":"compute-auto-scaling/","text":"EC2 Auto Scaling There are three main components to EC2 Auto Scaling. Launch template or configuration : What resource should be automatically scaled? EC2 Auto Scaling Group : Where should the resources be deployed? Scaling policies : When should the resources be added or removed? Launch Template You can create a launch template one of three ways. The fastest way to create a template is to use an existing EC2 instance . All the settings are already defined. Another option is to create one from an already existing template or a previous version of a launch template . The last option is to create a template from scratch . The following options will need to be defined: AMI ID, instance type, key pair, security group, storage, and resource tags. The launch template has version control . Auto Scaling Group To set the auto scaling group, we need to choose the launch template type of EC2 purchase, i.e. on Demand, Spot instances or a hybrid of the two. VPC and Subnets Elastic Load Balancer The next important task is to set the capacity. Scaling Policies We can add in CloudWatch an alarm state trigger to activate ACG to launch EC2 instances when a threshold is met.","title":"Auto Scaling"},{"location":"compute-auto-scaling/#ec2-auto-scaling","text":"There are three main components to EC2 Auto Scaling. Launch template or configuration : What resource should be automatically scaled? EC2 Auto Scaling Group : Where should the resources be deployed? Scaling policies : When should the resources be added or removed?","title":"EC2 Auto Scaling"},{"location":"compute-auto-scaling/#launch-template","text":"You can create a launch template one of three ways. The fastest way to create a template is to use an existing EC2 instance . All the settings are already defined. Another option is to create one from an already existing template or a previous version of a launch template . The last option is to create a template from scratch . The following options will need to be defined: AMI ID, instance type, key pair, security group, storage, and resource tags. The launch template has version control .","title":"Launch Template"},{"location":"compute-auto-scaling/#auto-scaling-group","text":"To set the auto scaling group, we need to choose the launch template type of EC2 purchase, i.e. on Demand, Spot instances or a hybrid of the two. VPC and Subnets Elastic Load Balancer The next important task is to set the capacity.","title":"Auto Scaling Group"},{"location":"compute-auto-scaling/#scaling-policies","text":"We can add in CloudWatch an alarm state trigger to activate ACG to launch EC2 instances when a threshold is met.","title":"Scaling Policies"},{"location":"compute-containers/","text":"Containers Orchestration AWS has two container orchestration services, Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS). They are conceptually similar to each other. ECS EKS An EC2 instance with the ECS Agent installed and configured is called a container instance It is called a worker node An ECS Container is called a task It is called a node ECS runs on AWS native technology It is run on top of Kubernetes","title":"Containers"},{"location":"compute-containers/#containers","text":"","title":"Containers"},{"location":"compute-containers/#orchestration","text":"AWS has two container orchestration services, Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS). They are conceptually similar to each other. ECS EKS An EC2 instance with the ECS Agent installed and configured is called a container instance It is called a worker node An ECS Container is called a task It is called a node ECS runs on AWS native technology It is run on top of Kubernetes","title":"Orchestration"},{"location":"compute-ec2/","text":"EC2 To select the operating system (OS) of the VM, we first need to select the Amazon Machine Image (AMI). This can be images created by AWS, community created, offered in the Marketplace, or even one created by yourself. There are many different EC2 instance types that are optimized to either memory , compute , accelerated compute (GPU), storage , and general purpose . The naming conventions of instance type & size are as such. We can also change the instance type for an instance which was already created. Pricing There are three types of EC2 pricing options. On Demand means that you only pay for when the instance is up, aka pay-as-you-go model. We can configure the instances to shutdown off office hours, resulting in significant savings. Reserved Instances means that you are reserving and paying for the instance on a 1-3 year term. It can be a significant discount (~40%) to On-Demand. Spot Instances take advantage of unused EC2 capacity in the AWS Cloud. They are available at up to a 90% discount compared to On-Demand prices.","title":"EC2"},{"location":"compute-ec2/#ec2","text":"To select the operating system (OS) of the VM, we first need to select the Amazon Machine Image (AMI). This can be images created by AWS, community created, offered in the Marketplace, or even one created by yourself. There are many different EC2 instance types that are optimized to either memory , compute , accelerated compute (GPU), storage , and general purpose . The naming conventions of instance type & size are as such. We can also change the instance type for an instance which was already created.","title":"EC2"},{"location":"compute-ec2/#pricing","text":"There are three types of EC2 pricing options. On Demand means that you only pay for when the instance is up, aka pay-as-you-go model. We can configure the instances to shutdown off office hours, resulting in significant savings. Reserved Instances means that you are reserving and paying for the instance on a 1-3 year term. It can be a significant discount (~40%) to On-Demand. Spot Instances take advantage of unused EC2 capacity in the AWS Cloud. They are available at up to a 90% discount compared to On-Demand prices.","title":"Pricing"},{"location":"compute-elb/","text":"Elastic Load Balancer Load balancing refers to the process of distributing tasks across a set of resources. A typical request for an application would start from the browser of the client. It\u2019s sent to a load balancer. Then, it\u2019s sent to one of the EC2 instances that hosts the application. The return traffic would go back through the load balancer and back to the client browser. Thus, the load balancer is directly in the path of the traffic. Benefits The fact that ELB can load balance to IP addresses means that it can work in a hybrid mode as well, where it also load balances to on-premises servers . ELB is highly available . The only option you have to ensure is that the load balancer is deployed across multiple Availability Zones. In terms of scalability, ELB automatically scales to meet the demand of the incoming traffic. It handles the incoming traffic and sends it to your backend application. Types There are 3 types of load balancers in AWS, Application Load Balancer , Network Load Balancer , and Gateway Load Balancer . The differences between Application & Network Load Balancers are as follows. Components There are three components in an ELB. Listeners : The client connects to the listener. This is often referred to as client-side. To define a listener, a port must be provided as well as the protocol, depending on the load balancer type. There can be many listeners for a single load balancer. Target groups : The backend servers, or server-side, is defined in one or more target groups. This is where you define the type of backend you want to direct traffic to, such as EC2 Instances, AWS Lambda functions, or IP addresses. Also, a health check needs to be defined for each target group. Rules : To associate a target group to a listener, a rule must be used. Rules are made up of a condition that can be the source IP address of the client and a condition to decide which target group to send the traffic to. Health checks are important to ensure that an instance is working fine before the load balancer directs traffic to it. Configuring ELB Below are the steps to build an Application Load Balancer. We need to set the listeners, and define the VPC, AZs and their corresponding subnets within each AZ. Second, we select the target group, and health check route. Last, we select the targets, which in this case are the ec2 instances in the earlier selected subnets.","title":"Elastic Load Balancer"},{"location":"compute-elb/#elastic-load-balancer","text":"Load balancing refers to the process of distributing tasks across a set of resources. A typical request for an application would start from the browser of the client. It\u2019s sent to a load balancer. Then, it\u2019s sent to one of the EC2 instances that hosts the application. The return traffic would go back through the load balancer and back to the client browser. Thus, the load balancer is directly in the path of the traffic.","title":"Elastic Load Balancer"},{"location":"compute-elb/#benefits","text":"The fact that ELB can load balance to IP addresses means that it can work in a hybrid mode as well, where it also load balances to on-premises servers . ELB is highly available . The only option you have to ensure is that the load balancer is deployed across multiple Availability Zones. In terms of scalability, ELB automatically scales to meet the demand of the incoming traffic. It handles the incoming traffic and sends it to your backend application.","title":"Benefits"},{"location":"compute-elb/#types","text":"There are 3 types of load balancers in AWS, Application Load Balancer , Network Load Balancer , and Gateway Load Balancer . The differences between Application & Network Load Balancers are as follows.","title":"Types"},{"location":"compute-elb/#components","text":"There are three components in an ELB. Listeners : The client connects to the listener. This is often referred to as client-side. To define a listener, a port must be provided as well as the protocol, depending on the load balancer type. There can be many listeners for a single load balancer. Target groups : The backend servers, or server-side, is defined in one or more target groups. This is where you define the type of backend you want to direct traffic to, such as EC2 Instances, AWS Lambda functions, or IP addresses. Also, a health check needs to be defined for each target group. Rules : To associate a target group to a listener, a rule must be used. Rules are made up of a condition that can be the source IP address of the client and a condition to decide which target group to send the traffic to. Health checks are important to ensure that an instance is working fine before the load balancer directs traffic to it.","title":"Components"},{"location":"compute-elb/#configuring-elb","text":"Below are the steps to build an Application Load Balancer. We need to set the listeners, and define the VPC, AZs and their corresponding subnets within each AZ. Second, we select the target group, and health check route. Last, we select the targets, which in this case are the ec2 instances in the earlier selected subnets.","title":"Configuring ELB"},{"location":"compute-intro/","text":"Introduction There are many compute services in AWS, but they can be generally divided into three types; Virtual Machines (Instances), Container Services , or Serverless . For VMs in AWS, this is called Elastic Compute Cloud ( EC2 ). A summary of all compute services can be found here . Types of Compute Services Other Compute Services","title":"Intro"},{"location":"compute-intro/#introduction","text":"There are many compute services in AWS, but they can be generally divided into three types; Virtual Machines (Instances), Container Services , or Serverless . For VMs in AWS, this is called Elastic Compute Cloud ( EC2 ). A summary of all compute services can be found here . Types of Compute Services Other Compute Services","title":"Introduction"},{"location":"compute-lambda/","text":"Lambda AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. We can upload our code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic. We can also set up your code to automatically trigger from over 200 AWS services and SaaS applications or call it directly from any web or mobile app. Execution Environment It takes time to set up an execution context and do the necessary \"bootstrapping\", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. This is known as cold start . After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. This is known as execution context reuse . The execution context is kept warm 15mins after the end of an execution, before cold starting again. To keep your lambda warm, we can have a scheduled job in Cloudwatch to involve the API every 15mins. This setting is also available in tools like zappa & serverless. To keep multiples of the same lambdas always warm, we can set a lambda provisioned concurrency setting , but there will be an add-on charge. Objects declared outside of the function's handler method remain initialized, providing additional optimization when the function is invoked again . For example, if your Lambda function establishes a database connection, instead of reestablishing the connection, the original connection is used in subsequent invocations. However, it is recommended to add logic in the code to check if a connection exists before creating one. Each execution context provides 512 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. For information on deployment limits, see AWS Lambda limits. Create Lambda Handler In a python handler, it includes the handler name (function name), event , which is the information passing from the request, and context , which contains AWS specific invocations or execution of your lambda, e.g. memory, cognito identity details etc. def handler_name ( event , context ): # do something return something Zip libraries & handler We will need to zip both python packages and the handler... More # Download dependencies into folder sudo pip install --target ./add-dragon-package boto3 # Zip up your code (with dependencies); zip file created yet cd add-dragon-package zip -r9 ${ OLDPWD } / pythonAddDragonFunction.zip . # Add python script to zip, zip file is generated at this stage cd .. zip -g pythonAddDragonFunction.zip addDragon.py Create Lambda Then upload the zip file using either... Using AWS CLI, or # Create Lambda Function aws lambda create-function --function-name add-dragon --runtime python3.6 --role <IAM ROLE ARN> --handler addDragon.addDragonToFile --publish --zip-file fileb://pythonAddDragonFunction.zip # Invoke Lambda Function aws lambda invoke --function-name add-dragon output.txt --payload file://newDragonPayload.json # Update Lambda Code aws lambda update-function-code --function-name add-dragon --zip-file fileb://pythonAddDragonFunction.zip --publish There are additional options to add on to the lambda's configuration when we are creating the function, otherwise they will be defaulted. Using management console, to upload the zip file. Adding and saving test requests. Attaching Lambda to API Gateway When we have created the Lambda function, we can attached it to our API Gateway by clicking the Integration Request. And selecting the relevant integration options. Basic Configs Some important settings for lambda include the memory, which also determines the number of CPU, timeout, and execution roles. Others include concurrency, which limits the number of lambdas that will be spun up at a given time, and environment variables, which your function can access. Permissions There are two types of permissions for lambda. The first is execution permissions , which defines what the lambda can do. This is set up in an IAM role. The most basic policy is AWSLambdaBasicExecutionRole , which is allows permission to upload logs to CloudWatch. Some other lambda configured policies are included here . The second type of permission is resource permissions , which defines what can invoke or manage your lambda functions. This is set by triggers. Triggers Lambda functions can be invoked by the push or pull models. For the push model, for example, someone can send a request to the API Gateway, which triggers the lambda to run. Alternatively, someone can upload a file to S3, and it triggers a lambda to process that file. For the pull model, for example, lambda will look for data in streaming or queue services like SQS, Kinesis, DynamoDB Streams, and trigger to pull the data into the function. This is configured through event source mappings. Version & Alias You can publish a new version of your AWS Lambda function when you create new or update existing functions. Each version of a lambda function gets it\u2019s own unique Amazon Resource Name (ARN). You can then use these ARNs to use different versions of the Lambda function for different purposes. You also have the ability to create aliases for AWS Lambda functions. Aliases are essentially pointers to one specific Lambda version.","title":"Lambda"},{"location":"compute-lambda/#lambda","text":"AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. We can upload our code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic. We can also set up your code to automatically trigger from over 200 AWS services and SaaS applications or call it directly from any web or mobile app.","title":"Lambda"},{"location":"compute-lambda/#execution-environment","text":"It takes time to set up an execution context and do the necessary \"bootstrapping\", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. This is known as cold start . After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. This is known as execution context reuse . The execution context is kept warm 15mins after the end of an execution, before cold starting again. To keep your lambda warm, we can have a scheduled job in Cloudwatch to involve the API every 15mins. This setting is also available in tools like zappa & serverless. To keep multiples of the same lambdas always warm, we can set a lambda provisioned concurrency setting , but there will be an add-on charge. Objects declared outside of the function's handler method remain initialized, providing additional optimization when the function is invoked again . For example, if your Lambda function establishes a database connection, instead of reestablishing the connection, the original connection is used in subsequent invocations. However, it is recommended to add logic in the code to check if a connection exists before creating one. Each execution context provides 512 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. For information on deployment limits, see AWS Lambda limits.","title":"Execution Environment"},{"location":"compute-lambda/#create-lambda","text":"","title":"Create Lambda"},{"location":"compute-lambda/#handler","text":"In a python handler, it includes the handler name (function name), event , which is the information passing from the request, and context , which contains AWS specific invocations or execution of your lambda, e.g. memory, cognito identity details etc. def handler_name ( event , context ): # do something return something","title":"Handler"},{"location":"compute-lambda/#zip-libraries-handler","text":"We will need to zip both python packages and the handler... More # Download dependencies into folder sudo pip install --target ./add-dragon-package boto3 # Zip up your code (with dependencies); zip file created yet cd add-dragon-package zip -r9 ${ OLDPWD } / pythonAddDragonFunction.zip . # Add python script to zip, zip file is generated at this stage cd .. zip -g pythonAddDragonFunction.zip addDragon.py","title":"Zip libraries &amp; handler"},{"location":"compute-lambda/#create-lambda_1","text":"Then upload the zip file using either... Using AWS CLI, or # Create Lambda Function aws lambda create-function --function-name add-dragon --runtime python3.6 --role <IAM ROLE ARN> --handler addDragon.addDragonToFile --publish --zip-file fileb://pythonAddDragonFunction.zip # Invoke Lambda Function aws lambda invoke --function-name add-dragon output.txt --payload file://newDragonPayload.json # Update Lambda Code aws lambda update-function-code --function-name add-dragon --zip-file fileb://pythonAddDragonFunction.zip --publish There are additional options to add on to the lambda's configuration when we are creating the function, otherwise they will be defaulted. Using management console, to upload the zip file. Adding and saving test requests.","title":"Create Lambda"},{"location":"compute-lambda/#attaching-lambda-to-api-gateway","text":"When we have created the Lambda function, we can attached it to our API Gateway by clicking the Integration Request. And selecting the relevant integration options.","title":"Attaching Lambda to API Gateway"},{"location":"compute-lambda/#basic-configs","text":"Some important settings for lambda include the memory, which also determines the number of CPU, timeout, and execution roles. Others include concurrency, which limits the number of lambdas that will be spun up at a given time, and environment variables, which your function can access.","title":"Basic Configs"},{"location":"compute-lambda/#permissions","text":"There are two types of permissions for lambda. The first is execution permissions , which defines what the lambda can do. This is set up in an IAM role. The most basic policy is AWSLambdaBasicExecutionRole , which is allows permission to upload logs to CloudWatch. Some other lambda configured policies are included here . The second type of permission is resource permissions , which defines what can invoke or manage your lambda functions. This is set by triggers.","title":"Permissions"},{"location":"compute-lambda/#triggers","text":"Lambda functions can be invoked by the push or pull models. For the push model, for example, someone can send a request to the API Gateway, which triggers the lambda to run. Alternatively, someone can upload a file to S3, and it triggers a lambda to process that file. For the pull model, for example, lambda will look for data in streaming or queue services like SQS, Kinesis, DynamoDB Streams, and trigger to pull the data into the function. This is configured through event source mappings.","title":"Triggers"},{"location":"compute-lambda/#version-alias","text":"You can publish a new version of your AWS Lambda function when you create new or update existing functions. Each version of a lambda function gets it\u2019s own unique Amazon Resource Name (ARN). You can then use these ARNs to use different versions of the Lambda function for different purposes. You also have the ability to create aliases for AWS Lambda functions. Aliases are essentially pointers to one specific Lambda version.","title":"Version &amp; Alias"},{"location":"db-dynamodb/","text":"DynamoDB DynamoDB is a serverless, non-relational database. It charges based on the usage of the table and the amount of data that you are storing in that table, not by the hour or by the second. Core Components In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes . DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility. Security DynamoDB offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.","title":"DynamoDB"},{"location":"db-dynamodb/#dynamodb","text":"DynamoDB is a serverless, non-relational database. It charges based on the usage of the table and the amount of data that you are storing in that table, not by the hour or by the second.","title":"DynamoDB"},{"location":"db-dynamodb/#core-components","text":"In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes . DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.","title":"Core Components"},{"location":"db-dynamodb/#security","text":"DynamoDB offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.","title":"Security"},{"location":"db-intro/","text":"Databases Amazon has many database offerings, from its traditional RDS, to other various purpose built databases. Databases by AWS Below are other more specific use-cases for various databases. Databases are usually classified to be either operational / transactional, or analytical (aka data warehouse).","title":"Intro"},{"location":"db-intro/#databases","text":"Amazon has many database offerings, from its traditional RDS, to other various purpose built databases. Databases by AWS Below are other more specific use-cases for various databases. Databases are usually classified to be either operational / transactional, or analytical (aka data warehouse).","title":"Databases"},{"location":"db-rds/","text":"RDS Relational Database Service (RDS) is a suite of relational databases provided by AWS. They run on EC2 instances under the hood. Commercial: Oracle, SQL Server Open Source: MySQL, PostgreSQL, MariaDB Cloud Native: Amazon Aurora The cloud native option, Amazon Aurora , is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL.","title":"RDS"},{"location":"db-rds/#rds","text":"Relational Database Service (RDS) is a suite of relational databases provided by AWS. They run on EC2 instances under the hood. Commercial: Oracle, SQL Server Open Source: MySQL, PostgreSQL, MariaDB Cloud Native: Amazon Aurora The cloud native option, Amazon Aurora , is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL.","title":"RDS"},{"location":"interact-boto3/","text":"Boto3 Boto3 is the AWS SDK for Python. To get started, you need to have python installed. pip install boto3 Boto3 Types Boto3 has two types of interfaces, the client and resource. The client interface is low level and provides 1-1 mapping with the AWS services' APIs, with the return responses in JSON. All service operations are supported by clients. The resource interface is a high level API, a wrapper for the client interface so that commands are more intuitive. Below is an example to download a file from S3. import boto3 def download_file ( bucket_name , origin_blob_path , dest_filename ): \"\"\"Download blob from S3 bucket. Args: bucket_name (str) origin_blob_path (str) dest_filename (str): destination filename \"\"\" s3 = boto3 . resource ( \"s3\" ) s3_bucket = s3 . Bucket ( bucket_name ) s3 . Bucket ( bucket_name ) . download_file ( origin_blob_path , dest_filename ) However, it only exposes a subset of AWS API, so functionalities might be limited , though we can assess the client interface in the resouce too as shown below. s3 = boto3 . resource ( \"s3\" ) . meta . client () Credentials To access each of the AWS services, we will need to pass our credentials either as arguments into the client or resource interfaces (not recommended!), or the SDK can detect them within the env variables, or lastly if u set them in your aws configure . S3 S3 charges for the bandwidth transferred out of S3 if its more than 1GB/mth. One of the ways to reduce this limit is to use S3 Select where you send a filter that","title":"Boto3"},{"location":"interact-boto3/#boto3","text":"Boto3 is the AWS SDK for Python. To get started, you need to have python installed. pip install boto3","title":"Boto3"},{"location":"interact-boto3/#boto3-types","text":"Boto3 has two types of interfaces, the client and resource. The client interface is low level and provides 1-1 mapping with the AWS services' APIs, with the return responses in JSON. All service operations are supported by clients. The resource interface is a high level API, a wrapper for the client interface so that commands are more intuitive. Below is an example to download a file from S3. import boto3 def download_file ( bucket_name , origin_blob_path , dest_filename ): \"\"\"Download blob from S3 bucket. Args: bucket_name (str) origin_blob_path (str) dest_filename (str): destination filename \"\"\" s3 = boto3 . resource ( \"s3\" ) s3_bucket = s3 . Bucket ( bucket_name ) s3 . Bucket ( bucket_name ) . download_file ( origin_blob_path , dest_filename ) However, it only exposes a subset of AWS API, so functionalities might be limited , though we can assess the client interface in the resouce too as shown below. s3 = boto3 . resource ( \"s3\" ) . meta . client ()","title":"Boto3 Types"},{"location":"interact-boto3/#credentials","text":"To access each of the AWS services, we will need to pass our credentials either as arguments into the client or resource interfaces (not recommended!), or the SDK can detect them within the env variables, or lastly if u set them in your aws configure .","title":"Credentials"},{"location":"interact-boto3/#s3","text":"S3 charges for the bandwidth transferred out of S3 if its more than 1GB/mth. One of the ways to reduce this limit is to use S3 Select where you send a filter that","title":"S3"},{"location":"interact-cli/","text":"AWS CLI The AWS Command Line Interface , AWS CLI for short, allows us to easily access AWS services in the terminal. Installation If you\u2019re using Amazon EC2 instances or AWS Cloud9, the tools are already installed for you. To install in your local machine, refer to this guide . Command Syntax # command syntax aws --<optional-command> <main-command> <subcommand> <parameters> # e.g. aws s3 mb s3://bucketname We can find more info in the online CLI help docs, or just type aws <optional: command> <optional: subcommand> help , e.g. aws s3 mb help if needed. Configure To make it easier to send commands without the need to enter the credentials everytime, we can set a default profile or a specific profile name with those variables. The 4 credentials are the access and secret keys , region , and output format (default is json ). AWS Configure Cmd Desc aws configure enter access, secret & region names, for default profile aws configure --profile <name> enter access, secret & region names, based on a specific name aws s3 ls --profile <name> enter commands based on profile These credentials are stored in ~/.aws/credentials .","title":"AWS CLI"},{"location":"interact-cli/#aws-cli","text":"The AWS Command Line Interface , AWS CLI for short, allows us to easily access AWS services in the terminal.","title":"AWS CLI"},{"location":"interact-cli/#installation","text":"If you\u2019re using Amazon EC2 instances or AWS Cloud9, the tools are already installed for you. To install in your local machine, refer to this guide .","title":"Installation"},{"location":"interact-cli/#command-syntax","text":"# command syntax aws --<optional-command> <main-command> <subcommand> <parameters> # e.g. aws s3 mb s3://bucketname We can find more info in the online CLI help docs, or just type aws <optional: command> <optional: subcommand> help , e.g. aws s3 mb help if needed.","title":"Command Syntax"},{"location":"interact-cli/#configure","text":"To make it easier to send commands without the need to enter the credentials everytime, we can set a default profile or a specific profile name with those variables. The 4 credentials are the access and secret keys , region , and output format (default is json ). AWS Configure Cmd Desc aws configure enter access, secret & region names, for default profile aws configure --profile <name> enter access, secret & region names, based on a specific name aws s3 ls --profile <name> enter commands based on profile These credentials are stored in ~/.aws/credentials .","title":"Configure"},{"location":"interact-cloud9/","text":"Cloud9 Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don\u2019t need to install files or configure your development machine to start new projects. Interface of Cloud9 IDE Starting Cloud9 Cloud9 runs on an EC2 instance, and charges based on the instance type and storage. If we choose the cheapest t2.micro , AWS estimates it to only be USD$2.05 per month. Free tier AWS (first year of account opening) has 750 free hours for EC2, so the costs should be either neligible or none at all. We should allow the cost saving settings to be on to prevent ballooning accidental costs. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance To close Cloud9 IDE, we need to go to the EC2 instance to shutdown. To start the instance, we can just go to Cloud9 service in management console and open the IDE. Credentials By default, Cloud9 has temporary credentials that will refresh every 5 mins. These credentials are the same as the IAM role that was used to create this Cloud9 instance, so the permissions to AWS resources are the same. To use a different credentials, we need to go to Cloud9 Settings > AWS Settings > Credentials > turn off AWS managed temporary credentials. Then in the terminal, AWS Configure & enter your other credentials. Switching off Temp Credentials File Uploads We can upload files & access the settings from the Welcome page.","title":"Cloud9"},{"location":"interact-cloud9/#cloud9","text":"Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don\u2019t need to install files or configure your development machine to start new projects. Interface of Cloud9 IDE","title":"Cloud9"},{"location":"interact-cloud9/#starting-cloud9","text":"Cloud9 runs on an EC2 instance, and charges based on the instance type and storage. If we choose the cheapest t2.micro , AWS estimates it to only be USD$2.05 per month. Free tier AWS (first year of account opening) has 750 free hours for EC2, so the costs should be either neligible or none at all. We should allow the cost saving settings to be on to prevent ballooning accidental costs. Choosing EC2 instance type & cost savings setting when launching a Cloud9 instance To close Cloud9 IDE, we need to go to the EC2 instance to shutdown. To start the instance, we can just go to Cloud9 service in management console and open the IDE.","title":"Starting Cloud9"},{"location":"interact-cloud9/#credentials","text":"By default, Cloud9 has temporary credentials that will refresh every 5 mins. These credentials are the same as the IAM role that was used to create this Cloud9 instance, so the permissions to AWS resources are the same. To use a different credentials, we need to go to Cloud9 Settings > AWS Settings > Credentials > turn off AWS managed temporary credentials. Then in the terminal, AWS Configure & enter your other credentials. Switching off Temp Credentials","title":"Credentials"},{"location":"interact-cloud9/#file-uploads","text":"We can upload files & access the settings from the Welcome page.","title":"File Uploads"},{"location":"interact-cloudformation/","text":"CloudFormation AWS CloudFormation allows us to deploy our AWS infrastructure stack inside a template file. This template file is then uploaded to CloudFormation build the stack. We can define the template in either a JSON or YAML format. Resources : Ec2Instance : Type : 'AWS::EC2::Instance' Properties : SecurityGroups : - !Ref InstanceSecurityGroup KeyName : mykey ImageId : '' InstanceSecurityGroup : Type : 'AWS::EC2::SecurityGroup' Properties : GroupDescription : Enable SSH access via port 22 SecurityGroupIngress : - IpProtocol : tcp FromPort : '22' ToPort : '22' CidrIp : 0.0.0.0/0 CloudFormation is similar to Terraform with the difference that Terraform can be used to serve multiple cloud providers, not just AWS.","title":"CloudFormation"},{"location":"interact-cloudformation/#cloudformation","text":"AWS CloudFormation allows us to deploy our AWS infrastructure stack inside a template file. This template file is then uploaded to CloudFormation build the stack. We can define the template in either a JSON or YAML format. Resources : Ec2Instance : Type : 'AWS::EC2::Instance' Properties : SecurityGroups : - !Ref InstanceSecurityGroup KeyName : mykey ImageId : '' InstanceSecurityGroup : Type : 'AWS::EC2::SecurityGroup' Properties : GroupDescription : Enable SSH access via port 22 SecurityGroupIngress : - IpProtocol : tcp FromPort : '22' ToPort : '22' CidrIp : 0.0.0.0/0 CloudFormation is similar to Terraform with the difference that Terraform can be used to serve multiple cloud providers, not just AWS.","title":"CloudFormation"},{"location":"manage-config/","text":"Config AWS Config helps to evaluate existing configurations of your resources against your desired configurations. Some examples of configurations include:","title":"Config"},{"location":"manage-config/#config","text":"AWS Config helps to evaluate existing configurations of your resources against your desired configurations. Some examples of configurations include:","title":"Config"},{"location":"manage-systems-manager/","text":"Systems Manager The AWS Systems Manager helps to manage multiple AWS resources including EC2, S3, RDS, etc. This includes automation, run command, inventory, and others stated below. Automation and Run Command Systems Manager have feature called document, whereby you can select some automation or commands to various AWS resources. Inventory Patch Manager The patch manager can deploy system and software packages to groups of EC2 instances and also on-premises Compliance This feature can scan our managed instances for patch compliance and configuration inconsistencies. Session Manager The session manager allows us to get a secure session into your instances without having to open the firewall rules to allow access to those other ports. This means that you don't need to use the secure shell, the SSH protocol or the remote desktop to access our instances. Parameter Store The parameter store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. Example of using parameter store to extract some variables. import boto3 ssm = boto3 . client ( 'ssm' , 'ap-southeast-1' ) bucket_name = ssm . get_parameter ( Name = 'dragon_data_bucket_name' , WithDecryption = False )[ 'Parameter' ][ 'Value' ] file_name = ssm . get_parameter ( Name = 'dragon_data_file_name' , WithDecryption = False )[ 'Parameter' ][ 'Value' ]","title":"Systems Manager"},{"location":"manage-systems-manager/#systems-manager","text":"The AWS Systems Manager helps to manage multiple AWS resources including EC2, S3, RDS, etc. This includes automation, run command, inventory, and others stated below.","title":"Systems Manager"},{"location":"manage-systems-manager/#automation-and-run-command","text":"Systems Manager have feature called document, whereby you can select some automation or commands to various AWS resources.","title":"Automation and Run Command"},{"location":"manage-systems-manager/#inventory","text":"","title":"Inventory"},{"location":"manage-systems-manager/#patch-manager","text":"The patch manager can deploy system and software packages to groups of EC2 instances and also on-premises","title":"Patch Manager"},{"location":"manage-systems-manager/#compliance","text":"This feature can scan our managed instances for patch compliance and configuration inconsistencies.","title":"Compliance"},{"location":"manage-systems-manager/#session-manager","text":"The session manager allows us to get a secure session into your instances without having to open the firewall rules to allow access to those other ports. This means that you don't need to use the secure shell, the SSH protocol or the remote desktop to access our instances.","title":"Session Manager"},{"location":"manage-systems-manager/#parameter-store","text":"The parameter store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. Example of using parameter store to extract some variables. import boto3 ssm = boto3 . client ( 'ssm' , 'ap-southeast-1' ) bucket_name = ssm . get_parameter ( Name = 'dragon_data_bucket_name' , WithDecryption = False )[ 'Parameter' ][ 'Value' ] file_name = ssm . get_parameter ( Name = 'dragon_data_file_name' , WithDecryption = False )[ 'Parameter' ][ 'Value' ]","title":"Parameter Store"},{"location":"models/","text":"AWS Models Shared Responsibility Model AWS is responsible for Security of the Cloud , which includes all the infrastructure. Customers are responsible for Security in the Cloud , which include things like operating system patches, IAM user management, client and server encryption. Well-Architected Framework AWS Well-Architected Framework describes the key concepts, design principles, and architectural best practices for designing and running workloads in the cloud. Operational Excellence Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures Security Implement a strong identity foundation Enable traceability Apply security at all layers Automate security best practices Protect data in transit and at rest Keep people away from data Prepare for security events Reliability Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity Manage change in automation Performance Efficiency Democratize advanced technologies Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy Cost Optimization Implement cloud financial management Adopt a consumption model Measure overall efficiency Stop spending money on undifferentiated heavy lifting Analyze and attribute expenditure","title":"Models"},{"location":"models/#aws-models","text":"","title":"AWS Models"},{"location":"models/#shared-responsibility-model","text":"AWS is responsible for Security of the Cloud , which includes all the infrastructure. Customers are responsible for Security in the Cloud , which include things like operating system patches, IAM user management, client and server encryption.","title":"Shared Responsibility Model"},{"location":"models/#well-architected-framework","text":"AWS Well-Architected Framework describes the key concepts, design principles, and architectural best practices for designing and running workloads in the cloud. Operational Excellence Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures Security Implement a strong identity foundation Enable traceability Apply security at all layers Automate security best practices Protect data in transit and at rest Keep people away from data Prepare for security events Reliability Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity Manage change in automation Performance Efficiency Democratize advanced technologies Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy Cost Optimization Implement cloud financial management Adopt a consumption model Measure overall efficiency Stop spending money on undifferentiated heavy lifting Analyze and attribute expenditure","title":"Well-Architected Framework"},{"location":"monitor-cloudtrail/","text":"CloudTrail Cloudtrail logs API activity for auditing. Auditing means to find out who did what or when for an AWS resource. All the event histories of our AWS services are automatically logged in CloudTrail. If we view the event, we can have a granular and traceable details about it.","title":"CloudTrail"},{"location":"monitor-cloudtrail/#cloudtrail","text":"Cloudtrail logs API activity for auditing. Auditing means to find out who did what or when for an AWS resource. All the event histories of our AWS services are automatically logged in CloudTrail. If we view the event, we can have a granular and traceable details about it.","title":"CloudTrail"},{"location":"monitor-cloudwatch/","text":"Cloudwatch Metrics Metrics are data about the performance of your systems. By default, many services provide free metrics called basic monitoring for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring for some resources, such as your Amazon EC2 instances, or publish your own application metrics. Logs CloudWatch Logs allows us to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. Some services are set up to send log data to CloudWatch Logs with minimal effort like Lambda. All we need to do is to give the Lambda function the correct IAM permissions to post logs to CloudWatch Logs. Other services like EC2 and even on-premise servers requires more effort. We have to first install and configure the CloudWatch Logs agent in the instance. Some logging terminology are as follows. Log event : A log event is a record of activity recorded by the application or resource being monitored, and it has a timestamp and an event message. Log stream : Log events are then grouped into log streams, which are sequences of log events that all belong to the same resource being monitored. For example, logs for an EC2 instance are grouped together into a log stream that you can then filter or query for insights. Log groups : Log streams are then organized into log groups. A log group is composed of log streams that all share the same retention and permissions settings. For example, if you have multiple EC2 instances hosting your application and you are sending application log data to CloudWatch Logs, you can group the log streams from each instance into one log group. This helps keep your logs organized. Dashboards We can display metrics or logs as charts in a unified dashboard for easy monitoring of our resources. Alarms We can set cloudwatch alarms when a metric exceeds a certain threshold for a period of time. An alarm has three possible states. OK : The metric is within the defined threshold. Everything appears to be operating like normal. ALARM : The metric is outside of the defined threshold. This could be an operational issue. INSUFFICIENT_DATA : The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state. Billing You can set billing alerts in Cloudwatch by following this resource . Note that billing metric data is stored in the US East (N. Virginia) Region and represents worldwide charges.","title":"CloudWatch"},{"location":"monitor-cloudwatch/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"monitor-cloudwatch/#metrics","text":"Metrics are data about the performance of your systems. By default, many services provide free metrics called basic monitoring for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring for some resources, such as your Amazon EC2 instances, or publish your own application metrics.","title":"Metrics"},{"location":"monitor-cloudwatch/#logs","text":"CloudWatch Logs allows us to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. Some services are set up to send log data to CloudWatch Logs with minimal effort like Lambda. All we need to do is to give the Lambda function the correct IAM permissions to post logs to CloudWatch Logs. Other services like EC2 and even on-premise servers requires more effort. We have to first install and configure the CloudWatch Logs agent in the instance. Some logging terminology are as follows. Log event : A log event is a record of activity recorded by the application or resource being monitored, and it has a timestamp and an event message. Log stream : Log events are then grouped into log streams, which are sequences of log events that all belong to the same resource being monitored. For example, logs for an EC2 instance are grouped together into a log stream that you can then filter or query for insights. Log groups : Log streams are then organized into log groups. A log group is composed of log streams that all share the same retention and permissions settings. For example, if you have multiple EC2 instances hosting your application and you are sending application log data to CloudWatch Logs, you can group the log streams from each instance into one log group. This helps keep your logs organized.","title":"Logs"},{"location":"monitor-cloudwatch/#dashboards","text":"We can display metrics or logs as charts in a unified dashboard for easy monitoring of our resources.","title":"Dashboards"},{"location":"monitor-cloudwatch/#alarms","text":"We can set cloudwatch alarms when a metric exceeds a certain threshold for a period of time. An alarm has three possible states. OK : The metric is within the defined threshold. Everything appears to be operating like normal. ALARM : The metric is outside of the defined threshold. This could be an operational issue. INSUFFICIENT_DATA : The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state. Billing You can set billing alerts in Cloudwatch by following this resource . Note that billing metric data is stored in the US East (N. Virginia) Region and represents worldwide charges.","title":"Alarms"},{"location":"monitor-health/","text":"Health Trusted Advisor Health Monitoring Personal Health Dashboard Service Health Dashboard Personalised Not Personalised Shows performance & availability of your own AWS resources Shows current availability of each AWS service Proactive notification of scheduled events -","title":"Health"},{"location":"monitor-health/#health","text":"","title":"Health"},{"location":"monitor-health/#trusted-advisor","text":"","title":"Trusted Advisor"},{"location":"monitor-health/#health-monitoring","text":"Personal Health Dashboard Service Health Dashboard Personalised Not Personalised Shows performance & availability of your own AWS resources Shows current availability of each AWS service Proactive notification of scheduled events -","title":"Health Monitoring"},{"location":"monitor-xray/","text":"X-Ray AWS X-Ray is used to help to monitor the performance & latency of distributed applications like EC2, ECS, Lambda, and Elastic BeanStalk. There is a need to integrate the X-Ray SDK with your application and also install the X-Ray agent.","title":"X-Ray"},{"location":"monitor-xray/#x-ray","text":"AWS X-Ray is used to help to monitor the performance & latency of distributed applications like EC2, ECS, Lambda, and Elastic BeanStalk. There is a need to integrate the X-Ray SDK with your application and also install the X-Ray agent.","title":"X-Ray"},{"location":"networking-ip/","text":"IP Address Internet Protocol Address is a numerical value used for host or network interface identification and location addressing. There are two notations of IP address, the IPv4 and IPv6. In AWS there are 3 types of IP addresses. Private IP Public IP Elastic IP Retained with instance stop Change when instance restart Associated to Private IP All instances will have Only Public instances have For Public IPs only - - Can be dissociated and linked to a different Private IP Free Free Charged only when associated but not used AWS Cloud Essentials Course","title":"IP"},{"location":"networking-ip/#ip-address","text":"Internet Protocol Address is a numerical value used for host or network interface identification and location addressing. There are two notations of IP address, the IPv4 and IPv6. In AWS there are 3 types of IP addresses. Private IP Public IP Elastic IP Retained with instance stop Change when instance restart Associated to Private IP All instances will have Only Public instances have For Public IPs only - - Can be dissociated and linked to a different Private IP Free Free Charged only when associated but not used AWS Cloud Essentials Course","title":"IP Address"},{"location":"networking-vpc/","text":"VPC Amazon Virtual Private Cloud ( VPC ) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. Terms Desc VPC An isolated network you create in the AWS cloud Subnet Smaller isolated networks inside VPC Internet Gateway Enable internet to VPC Route Tables Direct traffic at VPC or subnet VPC with Subnets IP & CIDR Notation An example of a IPv4 address is 192.168.1.30 . If we want to specify a range of IPs between 192.168.1.0 and 192.168.1.255, we can use the Classless Inter-Domain Routing (CIDR) notation, to represent as 192.168.1.0/24 The higher the number after the /, the smaller the number of IP addresses in your network . For example, a range of 192.168.1.0/24 is smaller than 192.168.1.0/16. When working with networks in the AWS Cloud, you choose your network size by using CIDR notation. In AWS, the smallest IP range you can have is /28 , which provides you 16 IP addresses. The largest IP range you can have is a /16 , which provides you with 65,536 IP addresses. Creating VPC Creating Subnet As mentioned to compartmentalize our traffic in our VPC, we can divide them into subnets. Internet Gateway A subnet can be public or private depending on whether it is assign to an internet gateway. To do that, we first create an internet gateway, and associate it with our VPC. NAT Gateway Sometimes we need our private instances to connect to the internet for patches, etc. To do that we need to create a NAT gateway, and configure the private route table to connect to the nat-gateway-id. Routing Tables A main route table is automatically create to route traffic to our VPC. However, we need further directions to guide the traffic to each subnet, for that, we can create custom route tables. First we association the table with our VPC. Then, we edit the route, and add in 0.0.0.0/0 meaning it can take and deliver traffic from anywhere, and assign it to our previously created internet gateway. Last, we edit the subnet associations and add the subnets we want internet access to. Below shows an overall diagram on setting up of the VPC, subnets, and internet gateway and routing tables. Security ACL Tables for Subnets Network ACL (Access Control List) is a firewall at the subnet level. A network ACL enables you to control what kind of traffic is allowed to enter or leave your subnet. You can configure this by setting up rules that define what you want to filter. Network ACL\u2019s are considered stateless , so you need to include both the inbound and outbound ports used for the protocol. If you don\u2019t include the outbound range, your server would respond but the traffic would never leave the subnet. Security Groups for EC2 Security group is a firewall you define for your EC2 instances. The default configuration of a security group blocks all inbound traffic and allows all outbound traffic. The default configuration of a security group blocks all inbound traffic and allows all outbound traffic. To enable internet access, we need to open ports 80 (HTTP) and 443 (HTTPS) . We only need to define inbound rules as security groups are stateful . Network ACL Security Group Subnet Firewall Instance Firewall Stateless Stateful Ordered Rules Non-Ordered Rules Allow or Deny Rules Only Allow Rules VPC Peering To connect between different VPCs, whether in different accounts or regions, we use VPC Peering. On-Premise to VPCs AWS VPN connects through the internet to AWS. AWS Direct Connect connects through a private network to AWS. AWS Transit Gateway is used to connect between different VPCs (like VPC Peering) to on-premise networks, including VPN and Direct Connect. AWS Outposts allows running AWS Services within on-premise.","title":"VPC"},{"location":"networking-vpc/#vpc","text":"Amazon Virtual Private Cloud ( VPC ) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. Terms Desc VPC An isolated network you create in the AWS cloud Subnet Smaller isolated networks inside VPC Internet Gateway Enable internet to VPC Route Tables Direct traffic at VPC or subnet VPC with Subnets","title":"VPC"},{"location":"networking-vpc/#ip-cidr-notation","text":"An example of a IPv4 address is 192.168.1.30 . If we want to specify a range of IPs between 192.168.1.0 and 192.168.1.255, we can use the Classless Inter-Domain Routing (CIDR) notation, to represent as 192.168.1.0/24 The higher the number after the /, the smaller the number of IP addresses in your network . For example, a range of 192.168.1.0/24 is smaller than 192.168.1.0/16. When working with networks in the AWS Cloud, you choose your network size by using CIDR notation. In AWS, the smallest IP range you can have is /28 , which provides you 16 IP addresses. The largest IP range you can have is a /16 , which provides you with 65,536 IP addresses.","title":"IP &amp; CIDR Notation"},{"location":"networking-vpc/#creating-vpc","text":"","title":"Creating VPC"},{"location":"networking-vpc/#creating-subnet","text":"As mentioned to compartmentalize our traffic in our VPC, we can divide them into subnets.","title":"Creating Subnet"},{"location":"networking-vpc/#internet-gateway","text":"A subnet can be public or private depending on whether it is assign to an internet gateway. To do that, we first create an internet gateway, and associate it with our VPC.","title":"Internet Gateway"},{"location":"networking-vpc/#nat-gateway","text":"Sometimes we need our private instances to connect to the internet for patches, etc. To do that we need to create a NAT gateway, and configure the private route table to connect to the nat-gateway-id.","title":"NAT Gateway"},{"location":"networking-vpc/#routing-tables","text":"A main route table is automatically create to route traffic to our VPC. However, we need further directions to guide the traffic to each subnet, for that, we can create custom route tables. First we association the table with our VPC. Then, we edit the route, and add in 0.0.0.0/0 meaning it can take and deliver traffic from anywhere, and assign it to our previously created internet gateway. Last, we edit the subnet associations and add the subnets we want internet access to. Below shows an overall diagram on setting up of the VPC, subnets, and internet gateway and routing tables.","title":"Routing Tables"},{"location":"networking-vpc/#security","text":"","title":"Security"},{"location":"networking-vpc/#acl-tables-for-subnets","text":"Network ACL (Access Control List) is a firewall at the subnet level. A network ACL enables you to control what kind of traffic is allowed to enter or leave your subnet. You can configure this by setting up rules that define what you want to filter. Network ACL\u2019s are considered stateless , so you need to include both the inbound and outbound ports used for the protocol. If you don\u2019t include the outbound range, your server would respond but the traffic would never leave the subnet.","title":"ACL Tables for Subnets"},{"location":"networking-vpc/#security-groups-for-ec2","text":"Security group is a firewall you define for your EC2 instances. The default configuration of a security group blocks all inbound traffic and allows all outbound traffic. The default configuration of a security group blocks all inbound traffic and allows all outbound traffic. To enable internet access, we need to open ports 80 (HTTP) and 443 (HTTPS) . We only need to define inbound rules as security groups are stateful . Network ACL Security Group Subnet Firewall Instance Firewall Stateless Stateful Ordered Rules Non-Ordered Rules Allow or Deny Rules Only Allow Rules","title":"Security Groups for EC2"},{"location":"networking-vpc/#vpc-peering","text":"To connect between different VPCs, whether in different accounts or regions, we use VPC Peering.","title":"VPC Peering"},{"location":"networking-vpc/#on-premise-to-vpcs","text":"AWS VPN connects through the internet to AWS. AWS Direct Connect connects through a private network to AWS. AWS Transit Gateway is used to connect between different VPCs (like VPC Peering) to on-premise networks, including VPN and Direct Connect. AWS Outposts allows running AWS Services within on-premise.","title":"On-Premise to VPCs"},{"location":"security-iam/","text":"IAM AWS Identity and Access Management ( IAM ) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. There are two important terms in security. The first is Authentication ; being verifying if someone is who they say they are because they logged in with the proper credentials. We can create new user login accounts for this. The second term is Authorization , where the policies assign permissions to allow or deny you access to actions for certain AWS resources. Security Models Shared Responsibility Model Both AWS and yourself have responsibilities in security. AWS are responsibility for security of the cloud, which include the global infrastructure, and software to the virtualization layer. You are responsible for the security in the cloud. Principle of Least Privilege An important security recommendation is the principle of least privilege, where we only assign policies to a resource or person based on what they require only. User Access Users have static credential keys; i.e. username, password, access keys, and secret keys. Root User When we first create a AWS account, it will be created as a root user, having full access over this account. It will be wise to create a user for future logins for security reasons, as well as setting an MFA for this root account. User Groups We can add new users and assign policy permissions to authorize these users. However, it is recommended instead to assign user groups with permissions to these groups, and add users to these user groups. This way, the groups are tied to specific job roles and make management easier. AWS Cloud Essentials Course Roles For AWS services requiring programmatic access to other services, a role based access needs to be created. An IAM role is an identity you can create that has specific permissions with credentials that are valid for short durations . Roles do not have static login credentials. Identity Provider If you have an organization that spans many employees and multiple AWS accounts, you may want your employees to sign in with a single credential. This can be done using a third-party identity provider, which uses AWS roles to grant access to each of these so-called federated user . We can also use AWS SSO , an IdP that lets your users sign in to a user portal with a single set of credentials. It then provides them access to all their assigned accounts and applications in one central location. ARN AWS Resource Names (ARN) are uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls. Every AWS service that you create have an ARN. JSON Policy Syntax There are various elements in a policy. Action indicates the various actions an AWS service can perform, Effect indicates to allow or deny the actions, while the Resource is the specific service (by ARN) that you launched that have those actions. Version element defines the version of the policy language. It specifies the language syntax rules that are needed by AWS to process a policy. To use all the available policy features, include \"Version\": \"2012-10-17\" before the \"Statement\" element in all your policies. Effect element specifies whether the statement will Allow or Deny access to a particular resource. Action element describes the type of action that should be allowed or denied. In the above policy, the action is \"*\". This is called a wildcard, and it is used to symbolize every action inside your AWS account. Resource element specifies the object or objects that the policy statement covers. In the policy example above, the resource is also the wildcard \"*\". This represents all resources inside your AWS console. Principal element specifies the user, account, service, or other entity that is allowed or denied access to a resource. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetLifecycleConfiguration\" , \"s3:GetBucketTagging\" , \"s3:GetInventoryConfiguration\" , \"s3:GetObjectVersionTagging\" , \"s3:GetBucketLogging\" , \"s3:GetAccelerateConfiguration\" , \"s3:GetBucketPolicy\" , \"s3:GetObjectVersionTorrent\" , \"s3:GetObjectAcl\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetBucketObjectLockConfiguration\" , \"s3:GetIntelligentTieringConfiguration\" , \"s3:GetBucketRequestPayment\" , \"s3:GetObjectVersionAcl\" , \"s3:GetObjectTagging\" , \"s3:GetMetricsConfiguration\" , \"s3:GetBucketOwnershipControls\" , \"s3:GetBucketPublicAccessBlock\" , \"s3:GetBucketPolicyStatus\" , \"s3:GetObjectRetention\" , \"s3:GetBucketWebsite\" , \"s3:GetBucketVersioning\" , \"s3:GetBucketAcl\" , \"s3:GetObjectLegalHold\" , \"s3:GetBucketNotification\" , \"s3:GetReplicationConfiguration\" , \"s3:PutObject\" , \"s3:GetObject\" , \"s3:GetObjectTorrent\" , \"s3:GetBucketCORS\" , \"s3:GetAnalyticsConfiguration\" , \"s3:GetObjectVersionForReplication\" , \"s3:GetBucketLocation\" , \"s3:GetObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::dragon-data-sy/*\" , \"arn:aws:s3:::dragon-data-sy\" ] } ] } This can be further shorterned by using the asterisk symbol. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:Get*\" ], \"Resource\" : [ \"arn:aws:s3:::dragon-data-sy/*\" , \"arn:aws:s3:::dragon-data-sy\" ] } ] }","title":"IAM"},{"location":"security-iam/#iam","text":"AWS Identity and Access Management ( IAM ) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. There are two important terms in security. The first is Authentication ; being verifying if someone is who they say they are because they logged in with the proper credentials. We can create new user login accounts for this. The second term is Authorization , where the policies assign permissions to allow or deny you access to actions for certain AWS resources.","title":"IAM"},{"location":"security-iam/#security-models","text":"","title":"Security Models"},{"location":"security-iam/#shared-responsibility-model","text":"Both AWS and yourself have responsibilities in security. AWS are responsibility for security of the cloud, which include the global infrastructure, and software to the virtualization layer. You are responsible for the security in the cloud.","title":"Shared Responsibility Model"},{"location":"security-iam/#principle-of-least-privilege","text":"An important security recommendation is the principle of least privilege, where we only assign policies to a resource or person based on what they require only.","title":"Principle of Least Privilege"},{"location":"security-iam/#user-access","text":"Users have static credential keys; i.e. username, password, access keys, and secret keys.","title":"User Access"},{"location":"security-iam/#root-user","text":"When we first create a AWS account, it will be created as a root user, having full access over this account. It will be wise to create a user for future logins for security reasons, as well as setting an MFA for this root account.","title":"Root User"},{"location":"security-iam/#user-groups","text":"We can add new users and assign policy permissions to authorize these users. However, it is recommended instead to assign user groups with permissions to these groups, and add users to these user groups. This way, the groups are tied to specific job roles and make management easier. AWS Cloud Essentials Course","title":"User Groups"},{"location":"security-iam/#roles","text":"For AWS services requiring programmatic access to other services, a role based access needs to be created. An IAM role is an identity you can create that has specific permissions with credentials that are valid for short durations . Roles do not have static login credentials.","title":"Roles"},{"location":"security-iam/#identity-provider","text":"If you have an organization that spans many employees and multiple AWS accounts, you may want your employees to sign in with a single credential. This can be done using a third-party identity provider, which uses AWS roles to grant access to each of these so-called federated user . We can also use AWS SSO , an IdP that lets your users sign in to a user portal with a single set of credentials. It then provides them access to all their assigned accounts and applications in one central location.","title":"Identity Provider"},{"location":"security-iam/#arn","text":"AWS Resource Names (ARN) are uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls. Every AWS service that you create have an ARN.","title":"ARN"},{"location":"security-iam/#json-policy-syntax","text":"There are various elements in a policy. Action indicates the various actions an AWS service can perform, Effect indicates to allow or deny the actions, while the Resource is the specific service (by ARN) that you launched that have those actions. Version element defines the version of the policy language. It specifies the language syntax rules that are needed by AWS to process a policy. To use all the available policy features, include \"Version\": \"2012-10-17\" before the \"Statement\" element in all your policies. Effect element specifies whether the statement will Allow or Deny access to a particular resource. Action element describes the type of action that should be allowed or denied. In the above policy, the action is \"*\". This is called a wildcard, and it is used to symbolize every action inside your AWS account. Resource element specifies the object or objects that the policy statement covers. In the policy example above, the resource is also the wildcard \"*\". This represents all resources inside your AWS console. Principal element specifies the user, account, service, or other entity that is allowed or denied access to a resource. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetLifecycleConfiguration\" , \"s3:GetBucketTagging\" , \"s3:GetInventoryConfiguration\" , \"s3:GetObjectVersionTagging\" , \"s3:GetBucketLogging\" , \"s3:GetAccelerateConfiguration\" , \"s3:GetBucketPolicy\" , \"s3:GetObjectVersionTorrent\" , \"s3:GetObjectAcl\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetBucketObjectLockConfiguration\" , \"s3:GetIntelligentTieringConfiguration\" , \"s3:GetBucketRequestPayment\" , \"s3:GetObjectVersionAcl\" , \"s3:GetObjectTagging\" , \"s3:GetMetricsConfiguration\" , \"s3:GetBucketOwnershipControls\" , \"s3:GetBucketPublicAccessBlock\" , \"s3:GetBucketPolicyStatus\" , \"s3:GetObjectRetention\" , \"s3:GetBucketWebsite\" , \"s3:GetBucketVersioning\" , \"s3:GetBucketAcl\" , \"s3:GetObjectLegalHold\" , \"s3:GetBucketNotification\" , \"s3:GetReplicationConfiguration\" , \"s3:PutObject\" , \"s3:GetObject\" , \"s3:GetObjectTorrent\" , \"s3:GetBucketCORS\" , \"s3:GetAnalyticsConfiguration\" , \"s3:GetObjectVersionForReplication\" , \"s3:GetBucketLocation\" , \"s3:GetObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::dragon-data-sy/*\" , \"arn:aws:s3:::dragon-data-sy\" ] } ] } This can be further shorterned by using the asterisk symbol. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:Get*\" ], \"Resource\" : [ \"arn:aws:s3:::dragon-data-sy/*\" , \"arn:aws:s3:::dragon-data-sy\" ] } ] }","title":"JSON Policy Syntax"},{"location":"step-functions/","text":"","title":"Step Functions"},{"location":"storage-ebs/","text":"Elastic Block Storage Amazon EBS is a block-level storage device that you can attach to an Amazon EC2 instance. These storage devices are called Amazon EBS volumes. They are similar to how we attach an external drive to our laptops. There are two main categories of Amazon EBS volumes: solid-state drives (SSDs) and hard-disk drives (HDDs). SSDs provide strong performance for random input/output (I/O), while HDDs provide strong performance for sequential I/O. AWS offers two types of each. Scaling Scaling is done in two ways * Increase volume size (max 16TB) * Attach multiple EC2 instances to volume Uses Operating systems : Boot/root volumes to store an operating system. The root device for an instance launched from an Amazon Machine Image (AMI) is typically an Amazon EBS volume. These are commonly referred to as EBS-backed AMIs. Databases : A storage layer for databases running on Amazon EC2 that rely on transactional reads and writes. Enterprise applications : Amazon EBS provides reliable block storage to run business-critical applications. Throughput-intensive applications : Applications that perform long, continuous reads and writes. Benefits High availability : When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss from single points of failure. Data persistence : The storage persists even when your instance doesn\u2019t. Data encryption : All EBS volumes support encryption. Flexibility : EBS volumes support on-the-fly changes. You can modify volume type, volume size, and input/output operations per second (IOPS) capacity without stopping your instance. Backups : Amazon EBS provides you the ability to create backups of any EBS volume. EBS Snapshots We can backup EBS by taking snapshots of it. This will be stored in multiple AZ within S3.","title":"EBS"},{"location":"storage-ebs/#elastic-block-storage","text":"Amazon EBS is a block-level storage device that you can attach to an Amazon EC2 instance. These storage devices are called Amazon EBS volumes. They are similar to how we attach an external drive to our laptops. There are two main categories of Amazon EBS volumes: solid-state drives (SSDs) and hard-disk drives (HDDs). SSDs provide strong performance for random input/output (I/O), while HDDs provide strong performance for sequential I/O. AWS offers two types of each.","title":"Elastic Block Storage"},{"location":"storage-ebs/#scaling","text":"Scaling is done in two ways * Increase volume size (max 16TB) * Attach multiple EC2 instances to volume","title":"Scaling"},{"location":"storage-ebs/#uses","text":"Operating systems : Boot/root volumes to store an operating system. The root device for an instance launched from an Amazon Machine Image (AMI) is typically an Amazon EBS volume. These are commonly referred to as EBS-backed AMIs. Databases : A storage layer for databases running on Amazon EC2 that rely on transactional reads and writes. Enterprise applications : Amazon EBS provides reliable block storage to run business-critical applications. Throughput-intensive applications : Applications that perform long, continuous reads and writes.","title":"Uses"},{"location":"storage-ebs/#benefits","text":"High availability : When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss from single points of failure. Data persistence : The storage persists even when your instance doesn\u2019t. Data encryption : All EBS volumes support encryption. Flexibility : EBS volumes support on-the-fly changes. You can modify volume type, volume size, and input/output operations per second (IOPS) capacity without stopping your instance. Backups : Amazon EBS provides you the ability to create backups of any EBS volume.","title":"Benefits"},{"location":"storage-ebs/#ebs-snapshots","text":"We can backup EBS by taking snapshots of it. This will be stored in multiple AZ within S3.","title":"EBS Snapshots"},{"location":"storage-intro/","text":"Storage AWS storage services are grouped into three different categories: block storage, file storage, and object storage. File Storage Each file has metadata such as file name, file size, and the date the file was created. The file also has a path, for example, computer/Application_files/Cat_photos/cats-03.png. When you need to retrieve a file, your system can use the path to find it in the file hierarchy. An example is Elastic File System ( EFS ). Block Storage While file storage treats files as a singular unit, block storage splits files into fixed-size chunks of data called blocks that have their own addresses. Since each block is addressable, blocks can be retrieved efficiently. Since block storage is optimized for low-latency operations, it is a typical storage choice for high-performance enterprise workloads, such as databases or enterprise resource planning (ERP) systems, that require low-latency storage. An example is Elastic Block Storge ( EBS ). Object Storage Objects, much like files, are also treated as a single unit of data when stored. However, unlike file storage, these objects are stored in a flat structure instead of a hierarchy. Each object is a file with a unique identifier. This identifier, along with any additional metadata, is bundled with the data and stored. With object storage, you can store almost any type of data, and there is no limit to the number of objects stored, making it easy to scale. Object storage is generally useful when storing large data sets, unstructured files like media assets, and static assets, such as photos. An example is Simple Storage Service ( S3 ).","title":"Intro"},{"location":"storage-intro/#storage","text":"AWS storage services are grouped into three different categories: block storage, file storage, and object storage.","title":"Storage"},{"location":"storage-intro/#file-storage","text":"Each file has metadata such as file name, file size, and the date the file was created. The file also has a path, for example, computer/Application_files/Cat_photos/cats-03.png. When you need to retrieve a file, your system can use the path to find it in the file hierarchy. An example is Elastic File System ( EFS ).","title":"File Storage"},{"location":"storage-intro/#block-storage","text":"While file storage treats files as a singular unit, block storage splits files into fixed-size chunks of data called blocks that have their own addresses. Since each block is addressable, blocks can be retrieved efficiently. Since block storage is optimized for low-latency operations, it is a typical storage choice for high-performance enterprise workloads, such as databases or enterprise resource planning (ERP) systems, that require low-latency storage. An example is Elastic Block Storge ( EBS ).","title":"Block Storage"},{"location":"storage-intro/#object-storage","text":"Objects, much like files, are also treated as a single unit of data when stored. However, unlike file storage, these objects are stored in a flat structure instead of a hierarchy. Each object is a file with a unique identifier. This identifier, along with any additional metadata, is bundled with the data and stored. With object storage, you can store almost any type of data, and there is no limit to the number of objects stored, making it easy to scale. Object storage is generally useful when storing large data sets, unstructured files like media assets, and static assets, such as photos. An example is Simple Storage Service ( S3 ).","title":"Object Storage"},{"location":"storage-s3/","text":"S3 AWS S3 stands for Simple Storage Service. SDK helper functions import boto3 s3 = boto3 . resource ( 's3' ) def upload_file ( s3 , local_filepath , bucketname , s3_filepath ): \"\"\"upload a file to S3\"\"\" s3 . meta . client . upload_file ( local_filepath , bucketname , s3_filepath ) print ( f \"upload { local_path } complete\" ) def download_file ( s3 , bucket_name , s3_filepath , local_filepath ): \"\"\"Download file from S3\"\"\" s3 . Bucket ( bucket_name ) . download_file ( s3_filepath , local_filepath ) print ( f \"download { s3_filepath } complete\" ) def query_json ( s3 , bucketname , SQL_expression , s3_filepath ) \"\"\"select query from json; filter query is done in S3 so its much cheaper\"\"\" SQL_expression = \"\"\"select * from S3Object[*][*] as s where s.family_str = 'blue' \"\"\" result = s3 . select_object_content ( Bucket = bucket_name , Key = file_name , ExpressionType = 'SQL' , Expression = SQL_expression , InputSerialization = { 'JSON' : { 'Type' : 'Document' }}, OutputSerialization = { 'JSON' : {}} return result if __name__ == \"__main__\" : local_path = \"/home/dragon_stats_one.json\" bucketname = \"dragon-data\" s3_path = \"dragon_stats_one.json\" upload_file ( s3 , local_path , bucketname , s3_path ) AWS CLI Note that all bucket URLs in aws cli must have the prefix with s3://bucket-name/example . More high level commands in the official documentation . Command Desc aws s3 ls <target> List buckets or files in a bucket etc. aws s3 cp <source> <target> Copy file from s3/local vice versa aws s3 mv <source> <target> move file a bucket location or to local aws s3 rm s3://<filepath> delete file in bucket aws s3 rm s3://<my-bucket/path> --recursive remove all files in directory","title":"S3"},{"location":"storage-s3/#s3","text":"AWS S3 stands for Simple Storage Service.","title":"S3"},{"location":"storage-s3/#sdk-helper-functions","text":"import boto3 s3 = boto3 . resource ( 's3' ) def upload_file ( s3 , local_filepath , bucketname , s3_filepath ): \"\"\"upload a file to S3\"\"\" s3 . meta . client . upload_file ( local_filepath , bucketname , s3_filepath ) print ( f \"upload { local_path } complete\" ) def download_file ( s3 , bucket_name , s3_filepath , local_filepath ): \"\"\"Download file from S3\"\"\" s3 . Bucket ( bucket_name ) . download_file ( s3_filepath , local_filepath ) print ( f \"download { s3_filepath } complete\" ) def query_json ( s3 , bucketname , SQL_expression , s3_filepath ) \"\"\"select query from json; filter query is done in S3 so its much cheaper\"\"\" SQL_expression = \"\"\"select * from S3Object[*][*] as s where s.family_str = 'blue' \"\"\" result = s3 . select_object_content ( Bucket = bucket_name , Key = file_name , ExpressionType = 'SQL' , Expression = SQL_expression , InputSerialization = { 'JSON' : { 'Type' : 'Document' }}, OutputSerialization = { 'JSON' : {}} return result if __name__ == \"__main__\" : local_path = \"/home/dragon_stats_one.json\" bucketname = \"dragon-data\" s3_path = \"dragon_stats_one.json\" upload_file ( s3 , local_path , bucketname , s3_path )","title":"SDK helper functions"},{"location":"storage-s3/#aws-cli","text":"Note that all bucket URLs in aws cli must have the prefix with s3://bucket-name/example . More high level commands in the official documentation . Command Desc aws s3 ls <target> List buckets or files in a bucket etc. aws s3 cp <source> <target> Copy file from s3/local vice versa aws s3 mv <source> <target> move file a bucket location or to local aws s3 rm s3://<filepath> delete file in bucket aws s3 rm s3://<my-bucket/path> --recursive remove all files in directory","title":"AWS CLI"}]}